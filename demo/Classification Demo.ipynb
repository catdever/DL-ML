{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec83ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from voltaml.compile import VoltaGPUCompiler\n",
    "from voltaml.inference import gpu_performance\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3892df",
   "metadata": {},
   "source": [
    "### Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9fd485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9960d1066ac4491a95a2bdfec215b2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "# model = torch.load('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91758d3d",
   "metadata": {},
   "source": [
    "## Set parameters for FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6b0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1,3,224,224)\n",
    "precision = 'fp16'\n",
    "compiled_model_dir = 'resnet50.engine' ## Set Model dir\n",
    "throughput_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4f365",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d078430b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:EngineBuilder:Network Description\n",
      "Network Description\n",
      "Network Description\n",
      "INFO:EngineBuilder:Input 'onnx::Conv_0' with shape (1, 3, 224, 224) and dtype DataType.FLOAT\n",
      "Input 'onnx::Conv_0' with shape (1, 3, 224, 224) and dtype DataType.FLOAT\n",
      "Input 'onnx::Conv_0' with shape (1, 3, 224, 224) and dtype DataType.FLOAT\n",
      "INFO:EngineBuilder:Output '495' with shape (1, 1000) and dtype DataType.FLOAT\n",
      "Output '495' with shape (1, 1000) and dtype DataType.FLOAT\n",
      "Output '495' with shape (1, 1000) and dtype DataType.FLOAT\n",
      "INFO:EngineBuilder:Building fp16 Engine in /workspace/voltaML/demo/resnet50.engine\n",
      "Building fp16 Engine in /workspace/voltaML/demo/resnet50.engine\n",
      "Building fp16 Engine in /workspace/voltaML/demo/resnet50.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/27/2022-09:39:42] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[07/27/2022-09:39:42] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1980, GPU 1938 (MiB)\n",
      "[07/27/2022-09:39:43] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2078, GPU 1950 (MiB)\n",
      "[07/27/2022-09:39:43] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2078, GPU 1958 (MiB)\n",
      "[07/27/2022-09:39:43] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:EngineBuilder:Serializing engine to file: /workspace/voltaML/demo/resnet50.engine\n",
      "Serializing engine to file: /workspace/voltaML/demo/resnet50.engine\n",
      "Serializing engine to file: /workspace/voltaML/demo/resnet50.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/27/2022-09:40:03] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[07/27/2022-09:40:04] [TRT] [I] Total Host Persistent Memory: 127232\n",
      "[07/27/2022-09:40:04] [TRT] [I] Total Device Persistent Memory: 47153664\n",
      "[07/27/2022-09:40:04] [TRT] [I] Total Scratch Memory: 4194304\n",
      "[07/27/2022-09:40:04] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 94 MiB, GPU 2385 MiB\n",
      "[07/27/2022-09:40:04] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 2.04392ms to assign 4 blocks to 62 nodes requiring 6201345 bytes.\n",
      "[07/27/2022-09:40:04] [TRT] [I] Total Activation Memory: 6201345\n",
      "[07/27/2022-09:40:04] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2125, GPU 2021 (MiB)\n",
      "[07/27/2022-09:40:04] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2125, GPU 2029 (MiB)\n",
      "[07/27/2022-09:40:04] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +45, GPU +64, now: CPU 90, GPU 128 (MiB)\n"
     ]
    }
   ],
   "source": [
    "compiler = VoltaGPUCompiler(\n",
    "    model=model,\n",
    "    output_dir=compiled_model_dir,\n",
    "    input_shape=input_shape,\n",
    "    precision=precision\n",
    ")\n",
    "\n",
    "compiled_model = compiler.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fdbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculating latency...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 214.66it/s]\n"
     ]
    },
    {
     "ename": "LogicError",
     "evalue": "cuMemcpyHtoD failed: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgpu_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthroughput_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthroughput_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/voltaML/voltaml/inference.py:134\u001b[0m, in \u001b[0;36mgpu_performance\u001b[0;34m(compiled_model, model, input_shape, throughput_batch_size, is_yolo)\u001b[0m\n\u001b[1;32m    132\u001b[0m gpu_inference_model \u001b[38;5;241m=\u001b[39m TensorRTInfer(compiled_model)\n\u001b[1;32m    133\u001b[0m torch_latency \u001b[38;5;241m=\u001b[39m measure_gpu_inference_latency(model, input_size\u001b[38;5;241m=\u001b[39minput_shape, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m voltaml_gpu_latency \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_gpu_inference_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_inference_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoltaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m torch_throughput \u001b[38;5;241m=\u001b[39m measure_gpu_inference_throughput(model, input_size\u001b[38;5;241m=\u001b[39minput_shape_for_throughput, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m voltaml_gpu_throughput \u001b[38;5;241m=\u001b[39m measure_gpu_inference_throughput(gpu_inference_model, input_size\u001b[38;5;241m=\u001b[39minput_shape_for_throughput, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoltaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/voltaML/voltaml/inference.py:330\u001b[0m, in \u001b[0;36mmeasure_gpu_inference_latency\u001b[0;34m(model, input_size, num_samples, num_warmups, model_type, is_yolo)\u001b[0m\n\u001b[1;32m    327\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m--> 330\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    332\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/workspace/voltaML/voltaml/trt_infer.py:94\u001b[0m, in \u001b[0;36mTensorRTInfer.infer\u001b[0;34m(self, batch, top)\u001b[0m\n\u001b[1;32m     91\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_spec())\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Process I/O and execute the network\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemcpy_htod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mexecute_v2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallocations)\n\u001b[1;32m     96\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_dtoh(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallocation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mLogicError\u001b[0m: cuMemcpyHtoD failed: invalid argument"
     ]
    }
   ],
   "source": [
    "gpu_performance(compiled_model_dir, model, input_shape=input_shape, throughput_batch_size=throughput_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341b48d",
   "metadata": {},
   "source": [
    "### Set parameters for INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1,3,224,224)\n",
    "precision = 'int8'\n",
    "compiled_model_dir = '' ## Compiled model directory\n",
    "throughput_batch_size = 1\n",
    "calib_input = '' ## Calib input images path\n",
    "calib_cache = '' ## Cache name\n",
    "calib_num_images=25000\n",
    "calib_batch_size=8\n",
    "calib_preprocessor='V2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966cfb9",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a64c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compiler = VoltaGPUCompiler(\n",
    "    model=model,\n",
    "    output_dir=compiled_model_dir,\n",
    "    input_shape=input_shape,\n",
    "    precision=precision,\n",
    "    calib_input=calib_input,\n",
    "    calib_cache=calib_cache,\n",
    "    calib_num_images=calib_num_images,\n",
    "    calib_batch_size=calib_batch_size,\n",
    "    calib_preprocessor=calib_preprocessor\n",
    ")\n",
    "\n",
    "compiled_model = compiler.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_performance(compiled_model_dir, model, input_shape=input_shape, throughput_batch_size=throughput_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3116e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a5daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422ab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
